import os
import numpy as np
import sys
import re
import math

BASE_DIR = '.'
GLOVE_DIR = BASE_DIR + '/glove.6B/'
TEXT_DATA_DIR = BASE_DIR + '/20newsbydate/20news-bydate-train/'
MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 20000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2
batch_size = 32
embeddings_index = {}


def putOutresult(content):
    f = open(os.path.join("output.txt"), 'w', encoding='utf-8')  # 文件操作
    f.write(content)


def load_glove():  # 暂时用不到
    print('Indexing word vectors.')
    f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8')
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    print('Found %s word vectors.' % len(embeddings_index))


def tf(word, count):  # word 文件
    return count[word] / sum(count.values())


def n_containing(word, count_list): #word 大字典
    return sum(1 for count in count_list if word in count_list[count])


# def n_containing(word, dictionary):  # word 和大字典  排序版本（元组）
#     sum=0
#     for document in dictionary:
#         for item in dictionary[document]:
#             if word==item[0]:
#                 sum=sum+1
#                 break
#     return sum
# return sum(1 for count in dictionary for count2 in count if word in count2)


def idf(word, count_list):  # word 和大字典
    return math.log(len(count_list)) / (1 + n_containing(word, count_list))


def tfidf(word, count, count_list): #word 文件 大字典
    return tf(word, count) * idf(word, count_list)


print('Processing text dataset')

texts = []
labels_index = {}
labels = []

if __name__ == '__main__':
    for name in sorted(os.listdir(TEXT_DATA_DIR)):
        path = os.path.join(TEXT_DATA_DIR, name)
        if os.path.isdir(path):
            label_id = len(labels_index)
            labels_index[name] = label_id  # 每个文件夹给一个ID
            for fname in sorted(os.listdir(path)):
                if fname.isdigit():
                    fpath = os.path.join(path, fname)
                    if sys.version_info < (3,):
                        f = open(fpath)
                    else:
                        f = open(fpath, encoding='latin-1')
                    texts.append(f.read().lower())
                    f.close()
                    labels.append(fname)
    print('Found %s texts.' % len(texts))
    print(labels)
    print(labels_index)
    # print(texts)

    reg = re.compile('\\W*')
    bigText = []
    for item in texts:
        try:
            bigText.append(reg.split(item))
        except:
            print("bigText is empty!")
            exit()
    # print(bigText)
    # print(len(labels))
    # print(len(texts))
    dictionary = {}  # 字典操作
    for document, lable in zip(bigText, labels):  # 注意中括号！！！
        dictionary[lable] = {}
        for word in document:
            if word in dictionary[lable]:
                dictionary[lable][word] = dictionary[lable][word] + 1
            else:
                dictionary[lable][word] = 1

    bigText = ''  # 垃圾回收
    # biglist = sorted(dictionary.items(), key=lambda item: item[1])  # lambda表达式

    # for document in dictionary:  #排序？
    #     dictionary[document] = sorted(dictionary[document].items(), key=lambda item: item[1])

    #putOutresult(str(dictionary))
    tfidfResult={}
    for document in dictionary:
        tfidfResult[document]={}
        for word in dictionary[document]:
            tfidfResult[document][word]=tfidf(word,dictionary[document],dictionary)
    putOutresult(str(tfidfResult))

    #
    #
    #
    # nb_words = min(MAX_NB_WORDS, len(biglist))
    # embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))
    # for word, i in biglist.items():
    #     if i > MAX_NB_WORDS:
    #         continue
    #     embedding_vector = embeddings_index.get(word)
    #     if embedding_vector is not None:
    #         embedding_matrix[i] = embedding_vector
    # print(embedding_matrix.shape)
    #
    # embedding_layer = Embedding(
    #     nb_words + 1,
    #     EMBEDDING_DIM,
    #     weights=[embedding_matrix],
    #     input_length=MAX_SEQUENCE_LENGTH,
    #     trainable=False,  # trainable，由于我们的W是word2vec训练出来的，算作预训练模型，所以就无需训练了。
    #     dropout=0.2
    # )
    # batch_size = 32
    # print('Build model...')
    #
    # model = Sequential()
    # model.add(embedding_layer)
    # model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))  # 输出维度 :100
    # model.add(Dense(1))
    # model.add(Activation('sigmoid'))
    # model.add(Dense(len(labels_index), activation='softmax'))
    #
    # model.compile(
    #     loss='binary_crossentropy',
    #     optimizer='adam',
    #     metrics=['accuracy']
    # )
    # print('Train...')
    # model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=5, validation_data=(x_val, y_val))
    #
    # score, acc = model.evaluate(x_val, y_val, batch_size=batch_size)
    #
    # print('Test score:', score)
